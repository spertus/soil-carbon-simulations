---
title: "SOC Measurement"
author: "Jake Spertus"
date: "May 20th, 2020"
output: html_notebook
header_includes:
  -\usepackage{amsmath}
  -\usepackage{amsfonts}
bibliography: soilcarbonstatistics.bib
---

```{r knitr setup, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r packages, message = FALSE, results = FALSE}
library(tidyverse)
source("sim_functions.R")
```

## Goals

This notebook serves to describe and implement a simulation to test the procedures currently used to measure soil organic carbon (SOC) against a known groundtruth. Questions to be answered include:

- With what accuracy can we recover true SOC on average (mean squared error)?
- Are estimates of SOC likely to center on the truth (bias)?
- Are estimates of uncertainty (standard errors) reflecting the true degree of uncertainty in the estimates (coverage of confidence intervals / type 1 error rate)? 

# Notation

### The Surface

To begin we will consider %SOC as a two-dimensional surface defined over plots $\mathcal{P} \subset \mathbb{R}^2$, where $\mathbb{R}^2$ is a vector space with the usual Euclidean norm $\|u\| = \sqrt{u_1^2 + u_2^2}$ and inner product $\langle u,v \rangle = u_1v_1 + u_2v_2$. Typically, $\mathcal{P} \equiv [a,b]\times[c,d]$, a rectangular plot. For example in [@ryals_impacts_2014] we have a $25 \times 60$ meter plot so that $|a-b| = 25$ and $|c-d| = 60$. A typical element of $\mathcal{P}$ is an ordered pair $(x,y)$, which denotes the position of a point in the plot. Since we typically will consider a single plot, we can define (0,0) to be the lower left hand corner of the plot, so that points $(x,y)$ are relative to the position of the plot itself in $\mathbb{R}^2$. WLOG, we'll typically also take the orientation of the plot to be such that the $y$ direction indicates North/South (latitude) and the $x$ direction indicates East/West (longitude). For example, $(1,2)$ indicates a point 1 meter to the east and 2 meters to the north of the origin. 

Given any point $(x,y)$ there is an associated concentration of soil carbon (%SOC), which we will denote by $z(x,y)$. Note that %SOC is not truly defined at a precise point $(x,y)$ - there is considerable variance at ever smaller scales: a tiny piece of root may be 100\% carbon, while a patch of sand may be 0\% carbon. $z(x,y)$ is better conceptualized as an average over a small window centered at $(x,y)$. In a design-based framework (as we are considering here), $z(x,y)$ is a fixed, unknown number. Typically the parameter of interest is:

$$\mu \equiv \int_\mathcal{P} z(x,y) d\mathcal{P}  = \int_a^b \int_c^d z(x,y)~ dy ~dx$$

This is the "population average" %SOC over the plot $\mathcal{P}$. We wish to estimate $\mu$ using data. In field studies, data take the form of finite samples determined at specific values of $x$ and $y$: $\{z(x_1,y_1), z(x_2, y_2), ... z(x_n, y_n) \}$ often we'll denote the data $\{z_1, z_2, ... z_n\} \equiv \{z_i\}_{i=1}^n$ when the locations themselves are not super important. 

The properties of the data $\{z_i\}_{i=1}^n$ and estimators based on them depend on how the sampling locations $(x_1,y_1),...,(x_n,y_n)$ were chosen. For example, we can easily get an unbiased estimator by simple random sampling, choosing $x_i \sim U[a,b]$ and $y_i \sim U[c,d]$ independently where $U[a,b]$ denotes the uniform distribution on $[a,b]$. Chosen in this way we have an unbiased sample mean:

$$\mathbb{E}\bigg [\frac{1}{n} \sum_{i=1}^n z_i \bigg ] = \mu$$

Other sampling mechanisms (e.g. stratified sampling) can also satisfy unbiasedness and may have lower variance, which is very important since collecting samples is expensive. 


### Depth

In reality, %SOC is a defined in 3-dimensions because soil has depth. For sequestration research in particular depth is an important consideration. We will let $d$ denote depth and set $d=0$ to be the soil surface so that it is always defined relatively (as opposed to say, altitude). With depth taken into account, $\mathcal{P} \subset \mathcal{R}^3$ and typically $\mathcal{P} = [a,b] \times [c,d] \times [e,f]$ with $|e - f| = 1$ meter. $z(x,y,d)$ is the %SOC at position $(x,y)$ and depth $d$. The parameter of interest (with apologies for overloading $d$) is:

$$\mu = \int_\mathcal{P} z(x,y,d) d\mathcal{P} = \int_a^b \int_c^d \int_e^f z(x,y,d) ~dd ~dy ~dx$$

A typical sampling design will *not* randomly sample $d \sim U[e,f]$. Typically, $d$ is specified in advance to cover a discrete grid of depths $\mathcal{D} \equiv \{d_1, d_2,...d_m\}$. For example $m = 4$ and  $\mathcal{D} = \{.1, .3, .5, 1.0\}$ meters. Further, there is typically no random sampling in depth. At *every* surface point $(x_i, y_i)$ we gather samples $\{z(x_i, y_i, d_1), z(x_i, y_i, d_2), ..., z(x_i, y_i, d_m)\}$. For brevity we'll denote the sample at position $(x_i, y_i)$ and depth $d_j$ by $z_i(d_j)$. Furthermore, when dealing with the surface $z_i(d_1)$ we will often supress depth completely, simply writing $z_i(d_1) \equiv z_i$



# Simulation

### The Surface

We start by simulating a two-dimensional surface $\{z(x, y) | (x,y) \in \mathcal{P} \subset \mathbb{R}^2\}$ as random draws from a distrubition, and then add additional depths deterministically.

The function $z(x,y)$ representing %SOC over the plot $\mathcal{P}$ should satisfy a few properties in order to be realistic. At the same time, we want to be able to randomly generate an arbitrarily large number of surfaces. We seek a distribution $\mathcal{F}$ so that $z(x,y,d) \sim \mathcal{F}$ will satisfy these properties. Namely $z(x,y,d)$ should:

- Have a user-specifiable mean and variance.
- Be bounded between 0 and 1 (0% and 100% SOC).
- $z(x_i,y_i)$ should be close to $z(x_j, y_j)$ when $(x_i,y_i)$ is close to $(x_j,y_j)$.

Points 1 and 2 could be handled naturally using a Beta distribution, but point 3 is trickier. In order to generate a correlated field we will first generate draws from a multivariate Gaussian with a covariance matrix that makes draws close when points are close (point 3). We will then squash these draws back to the interval $[0,1]$ using the Gaussian CDF $\Phi(\cdot)$. The correlation structure in the Gaussian is fixed by assuming stationarity and employing a particular variogram to define the covariance over distance. This is a standard technique in geostatistics, a model-based approach with applications in soil science (especially local estimation / the mapping of soils). 


In symbols we define $\mathcal{P} \subset \mathbb{R}^2$ as a discrete $I \times J$ grid and generate a random length-$(I\cdot J)$ vector $\boldsymbol{\zeta} \sim \mathcal{N}_{I\cdot J}(\theta, \Sigma)$. Then $Z(x_i, y_j) \equiv \Phi(\zeta_{i\cdot j})$. The nature of realizations $z(x_i, y_j)$ depend on the parameters $\theta$ and $\Sigma$. 

- $\theta$ determines where realizations will be centered. For example we may have 3\% SOC on average in a rangeland plot, giving $\theta = \Phi^{-1}(.03) \approx -1.9$. $\theta$ can also exhibit a trend in $(x_i, y_j)$. In this case we will write $\theta(x_i,y_j)$. If there is a linear trend we might write $\theta(x_i,y_j) \equiv \beta_0 + \beta_x x_i + \beta_y y_j$. This might happen if the plot lies on a slope, for example, wherein ridges tend to have less carbon than valleys. 
- $\Sigma$ determines the variability of $z(x_i, y_j)$ and how similar close points are to each other. 



Of course, variograms must themselves be estimated and variograms for SOC are kind of all over the place. [@paterson_variograms_2018] holds a compendium of variograms estimated from various studies of SOC, the vast majority of which were conducted on cropland. In this compilation, there are no SOC variograms on rangeland, and relatively few in the United States at all. I have decided to set the nugget variance equal to 0.005 and the sill variance equal to 0.02, with a range of 20 units (corresponding at a 10cm scale to 2m). This choice is poorly constrained to the best of my knowledge, but seems reasonable.

The idea here is to simulate the concentration of SOC in the topsoil (the first horizon, if you like) using a Gaussian copula and then extrapolate this to lower depths. In theory and in the field, we can sample anywhere on a plot so that %SOC comes from an infinite population. On a computer, we have a finite amount of memory to store a surface, so we must simulate it to some resolution. I have been using a $250 \times 600$ surface to represent a plot of land for sampling. If we take this as a standard 25m $\times$ 60m plot, we have a resolution of 10cm. We can consider a single point in the initial surface to represent the *average* %SOC in a $10 \times 10 \times 30$ centimeter cube. Note that 10 centimeters is a little bit larger than the diameter of a standard corer (7 cm in [@ryals_impacts_2014]).



```{r simulated surface}
source("sim_functions.R")


#Variogram input: Paterson et al 2018, "Variograms of soil properties..."
surface <- simulate_truth(size = c(250,600), nugget = .005, sill = .02, range = 20, intercept = .005, y_trend = TRUE, max_mean = .1)

#marginal histogram
sim_hist <- ggplot(data = surface, aes(z)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = mean(surface$z), size = 1.2, color = 'blue')
sim_hist

#surface
plot_surface(surface)
```


## Simulating depth

The above simulations took place in 2 dimensions. The generated surface can be thought of as the average density of SOC in the topsoil (say the first 0-30cm). For some goals (like enhanced productivity, tilth, water retention, etc) understanding concentrations in the top soil may be enough. For sequestration in particular, deeper soil horizons matter. Thus we must consider additional horizons for realistic simulations. 

A typical field study might sample from 4 depths: for example 0-10, 10-30, 30-50, and 50-100cm [@ryals_impacts_2014]. Evidence suggests that SOC decreases exponentially with depth [@allen_review_2010]. 

Let's generically define some number of horizons, say 4, that represent discrete horizons (e.g. as above 0-10,10-30,...) and identify these with indices 0,1,2,3 where 0 is the surface and 3 is the deepest horizon. Let $z(x,y,0)$ be the %SOC at the surface then the %SOC at deeper horizons is generated as: 

$$z(x,y,d) = z(x,y,0) \exp(\gamma d)$$

The parameter $\gamma$ determines how fast %SOC declines with depth (typically $\gamma < 0$ and larger $|\gamma|$ means a faster decline).


```{r}
#as an alternative to exponential decrease one can explicitly specify the dropoff by specifying decrease = "manual" and the proportions argument
surface_depth <- add_depth_samples(surface = surface, increments = 4, decrease = "exponential", gamma = -1)
boxplot(z ~ depth, data = surface_depth)
```




# Sampling 

For our purposes, key features of Silver lab sampling are:

- Plots are 60 by 25 meters.
- Samples are taken along a transect with $n = 9$ samples per plot (sometimes $n=5$ samples per plot). It is not clear if these are randomized at all or merely start at one corner, however I believe the sampling falls under the category of systematic random sampling, i.e. the initial location is random (see figure from Allen et al below).
- Occasionally, plots are stratified and multiple transects will be taken (e.g. 3 transects with 3 samples each)
- Soils are collected using a 7 cm corer, at 4 depths: 0-10 cm, 10-30 cm, 30-50 cm, and 50-100 cm.


## Upper bounds for simple random sampling

We can use the *Hoeffding bound* to construct a finite sample confidence interval on the percent carbon estimated using simple random sampling. This bound is finite-sample valid and does not rely on any other assumptions other than the fact that percent carbon is in $[0,1]$. For samples $X_1, X_2, ... X_n$ drawn from a bounded population, the probability that the sample mean is $\epsilon$ far from the population mean is:

$$P(|\bar{X} - \mathbb{E}[X_1]| \geq \epsilon) \leq 2\exp\{-2 n \epsilon^2\} $$

If we would like a 95% confidence interval on the sample mean, then we can set the LHS equal to .05 and rearrange:

\begin{align}
.05 &\leq 2 \exp\{- 2 n \epsilon^2\}\\ 
\log(.025) &\leq -2 n \epsilon^2\\
\frac{\log(.025)}{-2\epsilon^2} &\leq n
\end{align}

Thus for a 95\% confidence interval with width $\epsilon$ we would need $\frac{\log(.025)}{-2\epsilon^2}$ samples. This is a lot. Below we plot $n$ as a function of $\epsilon$, the desired precision, on the $\log_{10}$ scale.

On the other hand, we could consider the *central limit theorem* and an upper bound on the population variance. Since $X_i \in [0,1]$, we have that $\sigma^2 \leq 1/4$. The central limit theorem gives an asymptotic distribution of the variation of the sample mean from the population mean:

$$\mathbb{P}(|\bar{X} - \mu| > \epsilon) = 2 \bigg (1 - \Phi \big (\frac{\epsilon}{\sigma / \sqrt{n}} \big ) \bigg ) \leq 2 \big (1-\Phi ( 4\epsilon \sqrt{n}) \big )$$
Therefore if we wish to bound deviations from the mean by $\epsilon$ to occur with probability less than $\alpha$ we can collect $n$ so that:

\begin{align}
2(1 - \Phi(4 \epsilon \sqrt{n})) &\leq \alpha\\
&\implies \\
\Phi(4 \epsilon \sqrt{n}) \geq 1 - \alpha/2\\
&\implies\\
n \geq \frac{\Phi^{-1}(1-\alpha/2)^2}{16 \epsilon^2}
\end{align}

```{r}
get_n_hoeffding <- function(epsilon, alpha = .05){
  #compute the hoeffding bound given a level of precision (epsilon) and a level of desired confidence (alpha)
  n <- log(alpha / 2) / (-2 * epsilon^2) 
  n
}
#population variance is 1/4 in the worst case for pct carbon
get_n_clt <- function(epsilon, alpha = .05, variance = 1/4){
  #compute the sample sizes needed using a wald interval
  n <- (qnorm(1 - alpha/2)^2) / (16 * epsilon^2)
  n
}

epsilon_grid <- seq(.01, .1, length.out = 100)
n_vs_epsilon <- data.frame(n_hoeffding = get_n_hoeffding(epsilon_grid), n_clt = get_n_clt(epsilon_grid), epsilon = epsilon_grid) %>%
  pivot_longer(cols = c("n_hoeffding","n_clt"))



ggplot(data = n_vs_epsilon, aes(x = epsilon, y = value, colour = name)) +
  geom_line() +
  theme_linedraw() +
  scale_y_log10() +
  labs(y = "n", x = "epsilon")
```

# Compositing

Not every sample taken is measured for %SOC. Instead, soils are thoroughly blended together into a *composite* sample, which is then measured. The idea is to avoid the (heavy) labor costs of measurement, allowing more samples to be taken per plot. Theoretically, we could just composite $n$ soil samples down to 1 sample and measure it 1 time, yielding the sample mean we would get if we had measured all $n$ samples. In practice, this is a problem because of measurement error. We must measure at least a few times in order to get a good estimate of $\mu$ and the variance in the measurement error (if there is bias in measurement we may be out of luck; more on this below).  


Given samples $\{z_1,...z_n\}$ we bin them into $k$ groups of size $n/k$ (ideally $k$ should be a factor of $n$). The samples in each group are then physically mixed together to form composite samples $\{s_1, ... s_k\}$. Let $\{c_1,...,c_k\}$ be a set of sets, where each $c_i = \{c_{i1}, c_{i2},..., c_{i (n/k)}\}$ are the indices of the constituent samples of $s_i$. We assume that samples are mixed in equal proportions (*equal proportions compositing*), so that $s_i = \sum_{j \in c_i} \frac{k}{n} z_j$. Although we won't deal with it here, it is possible to get properties of composite samples that have been mixed without equal proportions of the constituent samples and an unbiased estimator with known variance can be recovered [@patil_composite_2011]. Crucially, the scientist still must know the proportions each soil was mixed in. A concern then is haphazardly compositing samples in the field. Compositing should be precise and mixing should be thorough. Additivity is another important assumption we've made, and is generally needed for valid compositing: the amount of SOC in the composited sample should equal the sum of the amount in the constituent samples. This assumption is met for SOC, but may be violated for other soil attributes, for example pH [@allen_review_2010]. 

Under equal proportions compositing the sample mean of the composite samples is an unbiased estimate of $\mu$:

$$\mathbb{E}\bigg [\frac{1}{k} \sum_{i=1}^k s_i  \bigg ] = \mathbb{E}\bigg [\frac{1}{k} \sum_{i=1}^k \sum_{j \in c_i} \frac{k}{n} z_j \bigg ] = \mathbb{E}\bigg [\frac{1}{n} \sum_{i=1}^n z_i  \bigg ] = \mu$$
Furthermore, because the sample mean of the composite samples is equivalent to the sample mean of the constiutents, it's variance is also 

$$\mathbb{V}\bigg[ \frac{1}{k} \sum_{i=1}^k s_i \bigg ] = \mathbb{V}\bigg [\frac{1}{n} \sum_{i=1}^n z_i \bigg ] = \frac{\sigma^2}{n}$$
where $\sigma^2$ is the population variance, i.e. the variance of $\{z(x,y) \mid (x,y) \in \mathcal{P} \}$. 

The subtly and potential pitfalls of compositing arise in the presence of measurement error, which we now discuss.



# Sample Preparation

SOC is typically analyzed in a lab, and there are considerable degrees of freedom in how samples are prepared for measurement once there. Labs adopt internal protocols to help eliminate these degrees of freedom, and there exist national and international standards to diminish variation between labs (e.g. [ISO](https://www.iso.org/standard/18782.html), [@burt_kellogg_2014]). Also, the costs of sample preparation comprise (along with measurement) a major bottleneck in field research, limiting the number of samples that can be analyzed.  

The Silver Lab uses a lab protocol like:

1) Collect soil cores.
2) Air dry samples
3) Sieve using a 2mm mesh screen
4) Pick roots 
5) Pulverize using a SPEX ball grinder
6) Check for carbonates with HCl
7) Measure using a Carbo Elantech elemental analyzer (dry combustion)

These steps help ensure that samples are free of water and homogenous. Especially if compositing has been done thorough mixing of equal proportion composites is essential. Steps 3 and 5 are instrumental to this. Dry combustion in an elemental analyzer is the gold standard for measurement of SOC. However, in the high heat of an elemental analyzer carbonates (an inorganic form of carbon) can be oxidized to CO$_2$ and appear as organic carbon. Thus, the application of HCl reveals the presence of carbonates which can throw off SOC measurement. 

Root picking is a potential source of bias in the laboratory. Roots are a major source of SOC but are not considered part of the soil matrix and so are picked out by some labs (e.g. the Silver lab). Root picking is done by humans and highly variable: some people may be very scrupulous and others may not be (anecdotally, I have heard this said in a lab). Especially in an experimental setting, giving one or multiple treatment plots to a scrupulous root picker and the control plots to a less scrupulous root picker could heavily bias treatment effect estimates.

I am not yet sure how to account for and/or model root picking in a software environment.



# Measurement

Once samples have been taken, error in the assay itself remains a concern. The Silver lab takes duplicate measurements until two runs are within 10% of each other (this often happens after the first two runs of any given sample). These percent differences are computed as:

$$\delta = 100 * \bigg | \frac{x_1 - x_2}{\frac{1}{2}(x_1 + x_2)} \bigg |$$



# Bulk Density


<!---
The total Carbon in a plot of land, in Megagrams Carbon per hectare (Mg C / ha), is computed from both the percent carbon and the bulk density (a measure of the density of the soil). The bulk density is itself difficult to sample and compute. For the time being we will treat this as fixed and known. From the Silver lab report for California's Fourth Climate Change Assessment, average bulk density across a number of Californian sites was about 1.2 grams per cubic centimeter (g / cm$^3$), which is the number I will use.

Typical carbon stocks: 

- 53 Mg C / ha observed in the Sierra foothills, as observed in [Ryals et al 2014](https://www.sciencedirect.com/science/article/abs/pii/S003807171300312X). In the [CCA4 report](https://www.energy.ca.gov/sites/default/files/2019-07/Agriculture_CCCA4-CNRA-2018-002.pdf)
- The Silver lab reported stocks of 11 to 108 Mg C / ha across a range of sites in California, with a mean stock of 27 Mg C / ha.
!--->


## Empirical Input: The Marin Data

Simulations will be based off of samples from Marin County collected by the Silver Lab at UC Berkeley.

```{r duplicate data, message = FALSE}
duplicate_data <- read_csv("mcp_duplicates.csv")
#drop samples that were rerun fewer or more than 2 times
no_dups <- names(which(table(duplicate_data$sample_name) < 2))
#also drop runs of atropine standards
duplicate_data <- duplicate_data %>%
  filter(!(sample_name %in% no_dups)) %>%
  filter(!(substr(sample_name, 1, 4) %in% c('atro','blan','Blan','dumm','Dumm'))) %>%
  group_by(sample_name) %>%
  filter(rank(time) <= 2)

duplicate_data
```

```{r pct carbon}
pct_carbon <- duplicate_data$carbon / 100
hist(pct_carbon, breaks = 30, xlab = "Proportion SOC")
```


# References 