---
title: "SOC Measurement"
author: "Jake Spertus"
date: "May 20th, 2020"
output: html_notebook
header_includes:
  -\usepackage{amsmath}
  -\usepackage{amsfonts}
  -\usepackage{color}
  -\newcommand{\indep}{\perp \!\!\! \perp}
bibliography: soilcarbonstatistics.bib
---

```{r knitr setup, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r packages, message = FALSE, results = FALSE}
library(tidyverse)
source("sim_functions.R")
```

## Goals

This notebook serves to describe and implement a simulation to test the procedures currently used to measure soil organic carbon (SOC) against a known groundtruth. Questions to be answered include:

- With what accuracy can we recover true SOC on average (mean squared error)?
- Are estimates of SOC likely to center on the truth (bias)?
- Are estimates of uncertainty (standard errors) reflecting the true degree of uncertainty in the estimates (coverage of confidence intervals / type 1 error rate)? 

# Notation

### The Surface

To begin we will consider %SOC as a two-dimensional surface defined over plots $\mathcal{P} \subset \mathbb{R}^2$, where $\mathbb{R}^2$ is a vector space with the usual Euclidean norm $\|u\| = \sqrt{u_1^2 + u_2^2}$ and inner product $\langle u,v \rangle = u_1v_1 + u_2v_2$. Typically, $\mathcal{P} \equiv [a,b]\times[c,d]$, a rectangular plot. For example in [@ryals_impacts_2014] we have a $25 \times 60$ meter plot so that $|a-b| = 25$ and $|c-d| = 60$. A typical element of $\mathcal{P}$ is an ordered pair $(x,y)$, which denotes the position of a point in the plot. Since we typically will consider a single plot, we can define (0,0) to be the lower left hand corner of the plot, so that points $(x,y)$ are relative to the position of the plot itself in $\mathbb{R}^2$. WLOG, we'll typically also take the orientation of the plot to be such that the $y$ direction indicates North/South (latitude) and the $x$ direction indicates East/West (longitude). For example, $(1,2)$ indicates a point 1 meter to the east and 2 meters to the north of the origin. 

Given any point $(x,y)$ there is an associated concentration of soil carbon (%SOC), which we will denote by $z(x,y)$. Note that %SOC is not truly defined at a precise point $(x,y)$ - there is considerable variance at ever smaller scales: a tiny piece of root may be 100\% carbon, while a patch of sand may be 0\% carbon. $z(x,y)$ is better conceptualized as an average over a small window centered at $(x,y)$. In a design-based framework (as we are considering here), $z(x,y)$ is a fixed, unknown number. Typically the parameter of interest is:

$$\mu \equiv \int_\mathcal{P} z(x,y) d\mathcal{P}  = \int_a^b \int_c^d z(x,y)~ dy ~dx$$

This is the "population average" %SOC over the plot $\mathcal{P}$. We wish to estimate $\mu$ using data. In field studies, data take the form of finite samples determined at specific values of $x$ and $y$: $\{z(x_1,y_1), z(x_2, y_2), ... z(x_n, y_n) \}$ often we'll denote the data $\{z_1, z_2, ... z_n\} \equiv \{z_i\}_{i=1}^n$ when the locations themselves are not super important. 

The properties of the data $\{z_i\}_{i=1}^n$ and estimators based on them depend on how the sampling locations $(x_1,y_1),...,(x_n,y_n)$ were chosen. For example, we can easily get an unbiased estimator by simple random sampling, choosing $x_i \sim U[a,b]$ and $y_i \sim U[c,d]$ independently where $U[a,b]$ denotes the uniform distribution on $[a,b]$. Chosen in this way we have an unbiased sample mean:

$$\mathbb{E}\bigg [\frac{1}{n} \sum_{i=1}^n z_i \bigg ] = \mu$$

Other sampling mechanisms (e.g. stratified sampling) can also satisfy unbiasedness and may have lower variance, which is very important since collecting samples is expensive. 


### Depth

In reality, %SOC is a defined in 3-dimensions because soil has depth. For sequestration research in particular depth is an important consideration. We will let $d$ denote depth and set $d=0$ to be the soil surface so that it is always defined relatively (as opposed to say, altitude). With depth taken into account, $\mathcal{P} \subset \mathcal{R}^3$ and typically $\mathcal{P} = [a,b] \times [c,d] \times [e,f]$ with $|e - f| = 1$ meter. $z(x,y,d)$ is the %SOC at position $(x,y)$ and depth $d$. The parameter of interest (with apologies for overloading $d$) is:

$$\mu = \int_\mathcal{P} z(x,y,d) d\mathcal{P} = \int_a^b \int_c^d \int_e^f z(x,y,d) ~dd ~dy ~dx$$

A typical sampling design will *not* randomly sample $d \sim U[e,f]$. Typically, $d$ is specified in advance to cover a discrete grid of depths $\mathcal{D} \equiv \{d_1, d_2,...d_m\}$. For example $m = 4$ and  $\mathcal{D} = \{.1, .3, .5, 1.0\}$ meters. Further, there is typically no random sampling in depth. At *every* surface point $(x_i, y_i)$ we gather samples $\{z(x_i, y_i, d_1), z(x_i, y_i, d_2), ..., z(x_i, y_i, d_m)\}$. For brevity we'll denote the sample at position $(x_i, y_i)$ and depth $d_j$ by $z_i(d_j)$. Furthermore, when dealing with the surface $z_i(d_1)$ we will often supress depth completely, simply writing $z_i(d_1) \equiv z_i$



# Simulation

### The Surface

We start by simulating a two-dimensional surface $\{z(x, y) | (x,y) \in \mathcal{P} \subset \mathbb{R}^2\}$ as random draws from a distrubition, and then add additional depths deterministically.

The function $z(x,y)$ representing %SOC over the plot $\mathcal{P}$ should satisfy a few properties in order to be realistic. At the same time, we want to be able to randomly generate an arbitrarily large number of surfaces. We seek a distribution $\mathcal{F}$ so that $z(x,y,d) \sim \mathcal{F}$ will satisfy these properties. Namely $z(x,y,d)$ should:

- Have a user-specifiable mean and variance.
- Be bounded between 0 and 1 (0% and 100% SOC).
- $z(x_i,y_i)$ should be close to $z(x_j, y_j)$ when $(x_i,y_i)$ is close to $(x_j,y_j)$.

Points 1 and 2 could be handled naturally using a Beta distribution, but point 3 is trickier. In order to generate a correlated field we will first generate draws from a multivariate Gaussian with a covariance matrix that makes draws close when points are close (point 3). We will then squash these draws back to the interval $[0,1]$ using the Gaussian CDF $\Phi(\cdot)$. The correlation structure in the Gaussian is fixed by assuming stationarity and employing a particular variogram to define the covariance over distance. This is a standard technique in geostatistics, a model-based approach with applications in soil science (especially local estimation / the mapping of soils). 


In symbols we define $\mathcal{P} \subset \mathbb{R}^2$ as a discrete $I \times J$ grid and generate a random length-$(I\cdot J)$ vector $\boldsymbol{\zeta} \sim \mathcal{N}_{I\cdot J}(\theta, \Sigma)$. Then $Z(x_i, y_j) \equiv \Phi(\zeta_{i\cdot j})$. The nature of realizations $z(x_i, y_j)$ depend on the parameters $\theta$ and $\Sigma$. 

- $\theta$ determines where realizations will be centered. For example we may have 3\% SOC on average in a rangeland plot, giving $\theta = \Phi^{-1}(.03) \approx -1.9$. $\theta$ can also exhibit a trend in $(x_i, y_j)$. In this case we will write $\theta(x_i,y_j)$. If there is a linear trend we might write $\theta(x_i,y_j) \equiv \beta_0 + \beta_x x_i + \beta_y y_j$. This might happen if the plot lies on a slope, for example, wherein ridges tend to have less carbon than valleys. 
- $\Sigma$ determines the variability of $z(x_i, y_j)$ and how similar close points are to each other. \textcolor{red}{Flesh this out more...}



Of course, variograms must themselves be estimated and variograms for SOC are kind of all over the place. [@paterson_variograms_2018] holds a compendium of variograms estimated from various studies of SOC, the vast majority of which were conducted on cropland. In this compilation, there are no SOC variograms on rangeland, and relatively few in the United States at all. I have decided to set the nugget variance equal to 0.005 and the sill variance equal to 0.02, with a range of 20 units (corresponding at a 10cm scale to 2m). This choice is poorly constrained to the best of my knowledge, but seems reasonable.

The idea here is to simulate the concentration of SOC in the topsoil (the first horizon, if you like) using a Gaussian copula and then extrapolate this to lower depths. In theory and in the field, we can sample anywhere on a plot so that %SOC comes from an infinite population. On a computer, we have a finite amount of memory to store a surface, so we must simulate it to some resolution. I have been using a $250 \times 600$ surface to represent a plot of land for sampling. If we take this as a standard 25m $\times$ 60m plot, we have a resolution of 10cm. We can consider a single point in the initial surface to represent the *average* %SOC in a $10 \times 10 \times 30$ centimeter cube. Note that 10 centimeters is a little bit larger than the diameter of a standard corer (7 cm in [@ryals_impacts_2014]).



```{r simulated surface}
source("sim_functions.R")


#Variogram input: Paterson et al 2018, "Variograms of soil properties..."
surface <- simulate_truth(size = c(250,600), nugget = .005, sill = .02, range = 20, intercept = .005, y_trend = TRUE, max_mean = .1)

#marginal histogram
sim_hist <- ggplot(data = surface, aes(z)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = mean(surface$z), size = 1.2, color = 'blue')
sim_hist

#surface
plot_surface(surface)
```


## Simulating depth

The above simulations took place in 2 dimensions. The generated surface can be thought of as the average density of SOC in the topsoil (say the first 0-30cm). For some goals (like enhanced productivity, tilth, water retention, etc) understanding concentrations in the top soil may be enough. For sequestration in particular, deeper soil horizons matter. Thus we must consider additional horizons for realistic simulations. 

A typical field study might sample from 4 depths: for example 0-10, 10-30, 30-50, and 50-100cm [@ryals_impacts_2014]. Evidence suggests that SOC decreases exponentially with depth [@allen_review_2010]. 

Let's generically define some number of horizons, say 4, that represent discrete horizons (e.g. as above 0-10,10-30,...) and identify these with indices 0,1,2,3 where 0 is the surface and 3 is the deepest horizon. Let $z(x,y,0)$ be the %SOC at the surface then the %SOC at deeper horizons is generated as: 

$$z(x,y,d) = z(x,y,0) \exp(\gamma d)$$

The parameter $\gamma$ determines how fast %SOC declines with depth (typically $\gamma < 0$ and larger $|\gamma|$ means a faster decline).


```{r}
#as an alternative to exponential decrease one can explicitly specify the dropoff by specifying decrease = "manual" and the proportions argument
surface_depth <- add_depth_samples(surface = surface, increments = 4, decrease = "exponential", gamma = -1)
boxplot(z ~ depth, data = surface_depth)
```




# Sampling 

We draw samples from a simulated surface using either simple random sampling or systematic random sampling. 

For simple random sampling pick $n$ sampling locations by drawing $X_i$ uniformly with replacement from $\{1,...,I\}$ and $Y_i$ uniformly with replacement from $\{1,...,J\}$. This picks out true samples $z(X_i, Y_i)$, where $z(X_i, Y_i)$ is random because $X_i$ and $Y_i$ are randomly chosen (not because $z(\cdot, \cdot)$ is a random fucntion). The data are realizations $z(x_i, y_i) \equiv z_i$.

For transect sampling, an initial we pick an initial $X_i$ from the left or right hand corner $X_i \sim \mbox{U}\{1,...a_{\mbox{limit}},b_{\mbox{limit}},...,I \}$ and an initial $Y_i$ from a band along the bottom of the plot $Y_i \sim \mbox{U}\{1,...c_{\mbox{limit}}\}$. The limits of the initial draw are chosen so that a regularly spaced transect can accomodate $n$ data points equally spaced across the diagonal of the plot. The rest of the points are then determined with no randomness by stepping along increments of the transect. Thus the sampling points and the samples SOC values $z(X_1,Y_1),...,z(X_n,Y_n)$ are random but not independent. 

For realizations $z(x_i,y_i)$ at the surface (topsoil), we have realizations at depth $z(x_i, y_i, d_j) \equiv z_i(d_j) = z(x_i,y_i) \cdot \exp(\gamma d_j)$, where again there is no randomness in $d_j$ (it is a deterministic grid).

```{r}
samples <- collect_sample(surface = surface_depth, design = "simple random sample", n_samp = 30)

topsoil_samples <- samples %>% filter(depth == 0) %>% pull(z)
```

# Compositing

Not every sample taken is measured for %SOC. Instead, soils are thoroughly blended together into a *composite* sample, which is then measured. The idea is to avoid the (heavy) labor costs of measurement, allowing more samples to be taken per plot. Theoretically, we could just composite $n$ soil samples down to 1 sample and measure it 1 time, yielding the sample mean we would get if we had measured all $n$ samples. In practice, this is a problem because of measurement error. We must measure at least a few times in order to get a good estimate of $\mu$ and the variance in the measurement error (if there is bias in measurement we may be out of luck; more on this below).  


Given samples $\{z_1,...z_n\}$ we bin them into $k$ groups of size $n/k$ (ideally $k$ should be a factor of $n$). The samples in each group are then physically mixed together to form composite samples $\{s_1, ... s_k\}$. Let $\{c_1,...,c_k\}$ be a set of sets, where each $c_i = \{c_{i1}, c_{i2},..., c_{i (n/k)}\}$ are the indices of the constituent samples of $s_i$. We assume that samples are mixed in equal proportions (*equal proportions compositing*), so that $s_i = \sum_{j \in c_i} \frac{k}{n} z_j$. Although we won't deal with it here, it is possible to get properties of composite samples that have been mixed without equal proportions of the constituent samples and an unbiased estimator with known variance can be recovered [@patil_composite_2011]. Crucially, the scientist still must know the proportions each soil was mixed in. A concern then is haphazardly compositing samples in the field. Compositing should be precise and mixing should be thorough. Additivity is another important assumption we've made, and is generally needed for valid compositing: the amount of SOC in the composited sample should equal the sum of the amount in the constituent samples. This assumption is met for SOC, but may be violated for other soil attributes, for example pH [@allen_review_2010]. 

Under equal proportions compositing the sample mean of the composite samples is an unbiased estimate of $\mu$:

$$\mathbb{E}\bigg [\frac{1}{k} \sum_{i=1}^k s_i  \bigg ] = \mathbb{E}\bigg [\frac{1}{k} \sum_{i=1}^k \sum_{j \in c_i} \frac{k}{n} z_j \bigg ] = \mathbb{E}\bigg [\frac{1}{n} \sum_{i=1}^n z_i  \bigg ] = \mu$$
Furthermore, because the sample mean of the composite samples is equivalent to the sample mean of the constiutents, it's variance is also 

$$\mathbb{V}\bigg[ \frac{1}{k} \sum_{i=1}^k s_i \bigg ] = \mathbb{V}\bigg [\frac{1}{n} \sum_{i=1}^n z_i \bigg ] = \frac{\sigma^2}{n}$$
where $\sigma^2$ is the population variance, i.e. the variance of $\{z(x,y) \mid (x,y) \in \mathcal{P} \}$. 

Why not always composite down to one sample and measure once? There are two reasons:

- There is measurement error that is (hopefully) unbiased but will be variable.
- For inference we must have an estimate of the population variance.

The first point could be addressed by compositing to 1 sample and measuring it some number of times -- say, 10 -- depending on how large the measurement error is. More on that below. 

The second point is non-negotiable. We need multiple measured samples from different locations on a plot in order to estimate the variance of our sample mean: $\hat{\mathbb{V}}[\frac{1}{n} \sum_{i=1}^n z_i] = \frac{\hat{\sigma}^2}{n}$. With only one measured sample $\hat{\sigma}^2$ is not estimable.


```{r}
composited_samples <- composite_samples(topsoil_samples, k = 5)
```


# Sample Preparation

SOC is typically analyzed in a lab, and there are considerable degrees of freedom in how samples are prepared for measurement once there. Labs adopt internal protocols to help eliminate these degrees of freedom, and there exist national and international standards to diminish variation between labs (e.g. [ISO](https://www.iso.org/standard/18782.html), [@burt_kellogg_2014]). Also, the costs of sample preparation comprise (along with measurement) a major bottleneck in field research, limiting the number of samples that can be analyzed.  

The Silver Lab uses a lab protocol like:

1) Collect soil cores.
2) Air dry samples in open plastic bags
3) Sieve using a 2mm mesh screen
4) Pick roots 
5) Pulverize using a SPEX ball grinder
6) Check for carbonates with HCl
7) Measure using a Carbo Elantech elemental analyzer (dry combustion)

These steps help ensure that samples are free of water and homogenous. Especially if compositing has been done thorough mixing of equal proportion composites is essential. Steps 3 and 5 are instrumental to this. Dry combustion in an elemental analyzer is the gold standard for measurement of SOC. However, in the high heat of an elemental analyzer carbonates (an inorganic form of carbon) can be oxidized to CO$_2$ and appear as organic carbon. Thus, the application of HCl reveals the presence of carbonates which can throw off SOC measurement. 

Root picking is a potential source of bias within and across labs. Not all labs root pick and, unlike 2mm sieving, it is not a widely held standard when defining and assessing SOC. Roots are a major source of SOC but are not considered part of the soil matrix and so are picked out by some labs (e.g. the Silver lab). Root picking is done by humans and highly variable: some people may be very scrupulous and others may not be (anecdotally, I have heard this said in a lab). Especially in an experimental setting, giving one or multiple treatment plots to a scrupulous root picker and the control plots to a less scrupulous root picker could heavily bias treatment effect estimates.

I am not yet sure how to account for and/or model root picking in the software.



# Measurement

Measurement error is a potentially major source of uncertainty in the analysis of SOC. A core can be assayed for %SOC in 4 main ways in a laboratory [@fao_measuring_2019]:

1) Wet digestion
2) Loss-on-ignition (LOI)
3) Spectroscopy
4) *Dry combustion in an elemental analyzer (DC-EA)*

I will focus on DC-EA since it is the gold standard and is in use in the Silver lab. First, a brief rundown of the other methods.


Wet digestion uses chemicals to oxidize and measure SOC. It is cheap and fast and is in common use worldwide. It is innacurate: only about ~75\% of SOC (with wide variation) is oxidized, so correction factors must be used (typically 1.33). It is generally not recommended [@fao_measuring_2019]. LOI is cheap but measures soil organic matter (SOM), not SOC. Typically, SOC represents between 43 adn 58\% of SOM, and these numbers are used as correction factors. LOI is just the difference between soil mass at 105 degrees C (so that water is boiled off) and 550 degrees C (so that SOM is oxidized). LOI typically overestimates the amount of SOM. It is difficult to standardize (especially across soil types) and therefore error prone. On the plus side, large sample masses can be analyzed (unlike in an elemental analyzer) and LOI is fairly cheap and widely accessible. In spectroscopic analysis, light is shone on the soil and its composition (including SOC) is revealed by the properties of the reflected light (typically in the visible, near infrared, or mid infrared spectra). SOC is predicted from spectral output, and an error is associated with the prediction. The label (i.e. the groundtruth \%SOC) is given by a standard method, for example wet digestion [@viscarra_rossel_visible_2006]. These may be used for rapid determination of SOC in large quantities / sample sizes. 

DC-EA is considered the gold standard for measurement. DC-EA measures %SOC burning a soil sample so that all the SOC is oxidized to CO$_2$, which is then measured. The EA takes in an aliquot of well-mixed soil that is placed into a tiny tin vessel. The mass of the aliquot depends on the machine but is typically very small, from 8 mg to a few grams [@fao_measuring_2019]. On the Silver lab's Carlo Elantech EA this is like 22 mg and must be measured extremely precisely (down to .001 mg, a microgram). EAs that can take larger samples may be less prone to error from tiny errors in the weight of the combusted aliquot. The little aliquot is dropped in its tin container into a combustion chamber that pipes in pure oxygen and burns the sample at $\approx 1000$ degrees celsius. This turns all the SOC into CO$_2$, which is then measured using gas chromatography. 

Although DC-EA is regarded as the best available method for quantification of %SOC, it is still prone to measurement error. The measurement error is likely driven by errors in the mass of the aliquot. 

Suppose we have $k$ (composite) samples with *true* %SOC content $\{s_1, ... s_k\}$ and let $\{s_1^*, ..., s^*_k \}$ be the *measured* value of our samples. Also suppose there are random measurement errors $\{\delta_1, ... , \delta_k \}$ that could be applied to each of our samples. There are three basic scenarios:

1) **No measurement error**: $s_i^* = s_i$
2) **Additive measurement error**: $s_i^* = s_i + \delta_i$
3) **Multiplicative measurement error**: $s_i^* = s_i \delta_i$

Under additive measurement error we need $\mathbb{E}[\delta_i] = 0$ for unbiasedness:

$$\mathbb{E}[s_i^*] =  \mathbb{E}[s_i + \delta_i] = \mathbb{E}[s_i] + \mathbb{E}[\delta_i] = \mu$$

Under multiplicative error we need $\mathbb{E}[\delta_i] = 1$ and $\delta_i \indep s_i$ (technically we just need $\mbox{Cov}(\delta_i, s_i) = 0$): 

$$\mathbb{E}[s_i^*] = \mathbb{E}[s_i\delta_i] = \mathbb{E}[s_i]\mathbb{E}[\delta_i] = \mu$$


I believe multiplicative measurement error is more reasonable because errors are likely to enter through errors in the mass, which is itself multiplicative in the determination of %SOC from a chromatographic reading. Also, multiplicative errors cannot yield negative SOC as long as $\delta_i \in \mathbb{R}_{\geq0}$. They could yield %SOC $> 1$, but for the range of %SOC and measurement error we are considering this won't happen.  


## Simulating Measurement error

We want to be able to simulate corrupting samples with measurement error. I would like to be able to simulate errors $\delta_i$ so that $\mathbb{E}[\delta_i] = 1$ or $\mathbb{E}[\delta_i] = 0$. I also want to fix the variance $\sigma_\delta^2$ and bounds $[a_\delta, b_\delta]$ of the distribution (note that it must be that $\sigma_\delta^2 \leq \frac{1}{4} (b_\delta - a_\delta)^2$). I will accomplish this by simulating $\delta_i$ from a centered and scaled beta distribution that is constrained to be symmetric. 

Here's how we will generate these samples given the parameters:

1) Draw $\delta^*_i \sim \mathcal{B}(\alpha,\alpha)$ where $\alpha = \left ( \frac{(a_\delta - b_\delta)^2}{8 \sigma_\delta^2} - \frac{1}{2} \right )$ 

2) Take 
$$\delta_i = \left (\delta_i^* - \frac{1}{2} \right ) |a_\delta - b_\delta| + \mathbb{E}[\delta_i]$$

For example if we want an error centered at 1 with bounds $[.8,1.2]$ and standard deviation $\sigma_\delta = .05$ then $\alpha = .4^2 / (8 * .05^2) - 1/2 = 7.5$ and $\delta_i = (\delta_i^* - 1/2)(.4) + 1$

```{r}
#measure once
measured_composite_samples <- perturb_measurements(true_samples = composited_samples, error_type = "multiplicative", error_bounds = c(.8, 1.2), error_sd = .05)

#measure_multiple_times (replicate the assay)
measured_composite_samples <- perturb_measurements(true_samples = composited_samples, error_type = "multiplicative", error_bounds = c(.8,1.2), error_sd = .05, replicates = 3) 
```

#### EcoCore and measurement error

[EcoCore analytic services](https://ecocore.nrel.colostate.edu/ecocore-elemental-analysis.html) reports a measurement error from DC-EA as a 2.5\% coefficient of variation assuming a mean %SOC of 2.5%. 


#### The Silver lab and measurement error

The Silver lab takes duplicate measurements until two runs are within 10% of each other. These percent differences are computed as:

$$\delta = 100 * \bigg | \frac{s_1 - s_2}{\frac{1}{2}(s_1 + s_2)} \bigg |$$

Therefore the measurement error of their samples is bounded by $\pm 10\%$. This is the interval I will use as a default in my simulations.


# Estimation and Inference

Recall that our overall goal is to estimate $\mu = \int_\mathcal{P} z(x,y,d) d\mathcal{P}$ using our measurements $\{s_i^*\}_{i=1}^k$. The sample mean of the measured composite samples is an unbiased estimator of $\mu$:
\begin{align}
\mathbb{E}\left [\frac{1}{k} \sum_{i=1}^k s_i^* \right ] &=  
\mathbb{E}\left [\frac{1}{k} \sum_{i=1}^k s_i\delta_i \right ]\\ 
&=  \mathbb{E}\left [\frac{1}{k} \sum_{i=1}^k \left [ \sum_{j \in c_i} \frac{k}{n} z_j \right ] \delta_i \right ]\\
&= \frac{1}{k} \sum_{i=1}^k \left [ \sum_{j \in c_i} \frac{k}{n}\mathbb{E}[z_j] \right ] \mathbb{E}[\delta_i]\\
&= \frac{1}{k} \sum_{i=1}^k \left [ \sum_{j \in c_i} \frac{k}{n}\mu \right ] \\
&= \frac{1}{k} \sum_{i=1}^k \mu \\
&= \mu 
\end{align}

Furthermore, its variance is:

\begin{align}
\mathbb{V}\left [\frac{1}{k} \sum_{i=1}^k s_i^* \right ] &= \frac{1}{k^2} \sum_{i=1}^k \mathbb{V}[s_i^*]\\
&=  \frac{1}{k^2} \sum_{i=1}^k \mathbb{V}[s_i \delta_i]\\
&= \frac{1}{k^2}  \sum_{i=1}^k \left ( \mathbb{V}[s_i] \mathbb{V}[\delta_i] + \mathbb{E}[s_i]^2 \mathbb{V}[\delta_i] + \mathbb{E}[\delta_i]^2 \mathbb{V}[s_i] \right )\\
&= \frac{1}{k^2}  \sum_{i=1}^k \left ( \mathbb{V}[s_i] \sigma_\delta^2 + \mu^2 \sigma_\delta^2 + \mathbb{E}[\delta_i]^2 \mathbb{V}[s_i] \right )\\
&= \frac{1}{k^2}  \sum_{i=1}^k \left ( \mathbb{V} \left [\sum_{j\in c_i} \frac{k}{n} z_i \right ] \sigma_\delta^2 + \mu^2 \sigma_\delta^2 + \mathbb{V}\left [\sum_{j\in c_i} \frac{k}{n} z_i \right ] \right )\\
&= \frac{1}{k^2}  \sum_{i=1}^k \left ( \left [ \sum_{j\in c_i} \frac{k^2}{n^2} \mathbb{V}[z_i] \right ] \sigma_\delta^2 + \mu^2 \sigma_\delta^2 + \left [ \sum_{j\in c_i} \frac{k^2}{n^2} \mathbb{V}[z_i] \right ] \right )\\
&= \frac{1}{k^2}  \sum_{i=1}^k \left ( \frac{k}{n} \sigma^2 \sigma_\delta^2 + \mu^2 \sigma_\delta^2 +  \frac{k}{n} \sigma^2\right )\\
&= \frac{1}{k} \left ( \frac{k}{n} \sigma^2 \sigma_\delta^2 + \mu^2 \sigma_\delta^2 +  \frac{k}{n} \sigma^2\right ) \\ 
&=  \frac{\sigma^2 (1 + \sigma_\delta^2) }{n}  + \frac{\mu^2 \sigma_\delta^2}{k}\\ 
\end{align}

First of all, if there is no measurement error than the variance of composited samples is just equal to $\sigma^2 / n$, i.e. the usual variance of a sample mean. Furthermore, the variance can be reduced in two ways: either by collecting more total samples (which reduces the first component) or by doing less compositing and thereby measuring more samples (which reduces the second component). 

In order to construct a confidence interval for our measured sample mean $\bar{s}^* = \frac{1}{k} \sum_{i=1}^k s_i^*$ we have to have an estimate of it's variance. That is, we want $\hat{\mathbb{V}}\left [\frac{1}{k} \sum_{i=1}^k s_i^* \right ]$. A plug-in estimator is:

$$\hat{\mathbb{V}}\left [\frac{1}{k} \sum_{i=1}^k s_i^* \right ] = \frac{\hat{\sigma}^2 (1 + \hat{\sigma}_\delta^2)}{n} + \frac{\hat{\mu}^2 \hat{\sigma}_\delta^2}{k}$$

How can we estimate or at least bound $\sigma^2$ and $\sigma_\delta^2$? 

<!---
Consider:

\begin{align}
\frac{1}{k-1} \sum_{i=1}^k (s_i^* - \bar{s}^*)^2 &= \frac{1}{k-1} \sum_{i=1}^k  \left(  \frac{k\delta_i}{n} \sum_{j \in c_i} z_j  - \sum_{l=1}^k   \frac{\delta_l}{n} \sum_{j \in c_l} z_j \right )^2\\
&= \frac{1}{k-1} \sum_{i=1}^k  \left(  \frac{k\delta_i}{n} \sum_{j \in c_i} z_j  - \left (\frac{\delta_i}{n} \sum_{j \in c_i} z_j + \sum_{l\neq i}   \frac{\delta_l}{n} \sum_{j \in c_l} z_j \right ) \right )^2 \\ 
&= \frac{1}{k-1} \sum_{i=1}^k  \left(  \frac{(k-1)\delta_i}{n} \sum_{j \in c_i} z_j  - \sum_{l\neq i}   \frac{\delta_l}{n} \sum_{j \in c_l} z_j \right )^2 \\ 
\end{align}
!--->

# Bulk Density

Soil bulk density is the mass per unit volume of the soil. It reflects the structure of the soil, including porosity and solids, and depends on the proportions of minerals in the soil, organic matter content, chemical composition, etc. These in turn reflect soil genesis, interaction of soil components, land use, management, etc. It's typically specified expressed in Mg m$^{-3}$ or equivalently in g cm$^{-3}$. Analyes should be in triplicate and account for/record the water status at the time of measurement [@fao_measuring_2019], e.g. whether stocks were computed shortly after a rain when the soil is wet and bulk density relatively higher. Further, bulk density should be measured on the *same core that %SOC is measured on* [@fao_measuring_2019].

Measurement methods

- **Undisturbed (intact) core method**: collect a known volume of soil using a metal ring pressed into the soil and determining the weight after drying. The core must be kept intact, which is difficult if the soil is too dry.
- **Excavation method**: excavate some quantity of soil, dry and weight it. Then determine its volume by filling it with sand of known volume per unit mass. 
- **Pedotransfer functions (PTFs)**: PTFs are not a measurement method per se. They are predictions of BD based on more easily measurable properties, like soil clay content and SOC content. They can lead to high uncertainty and errors in prediction need to be taken into account.




<!---
The total Carbon in a plot of land, in Megagrams Carbon per hectare (Mg C / ha), is computed from both the percent carbon and the bulk density (a measure of the density of the soil). The bulk density is itself difficult to sample and compute. For the time being we will treat this as fixed and known. From the Silver lab report for California's Fourth Climate Change Assessment, average bulk density across a number of Californian sites was about 1.2 grams per cubic centimeter (g / cm$^3$), which is the number I will use.

Typical carbon stocks: 

- 53 Mg C / ha observed in the Sierra foothills, as observed in [Ryals et al 2014](https://www.sciencedirect.com/science/article/abs/pii/S003807171300312X). In the [CCA4 report](https://www.energy.ca.gov/sites/default/files/2019-07/Agriculture_CCCA4-CNRA-2018-002.pdf)
- The Silver lab reported stocks of 11 to 108 Mg C / ha across a range of sites in California, with a mean stock of 27 Mg C / ha.
!--->


# Empirical Inputs:


### The Marin Data

Simulations will be based off of samples from Marin County collected by the Silver Lab at UC Berkeley.

```{r duplicate data, message = FALSE}
duplicate_data <- read_csv("mcp_duplicates.csv")
#drop samples that were rerun fewer or more than 2 times
no_dups <- names(which(table(duplicate_data$sample_name) < 2))
#also drop runs of atropine standards
duplicate_data <- duplicate_data %>%
  filter(!(sample_name %in% no_dups)) %>%
  filter(!(substr(sample_name, 1, 4) %in% c('atro','blan','Blan','dumm','Dumm'))) %>%
  group_by(sample_name) %>%
  filter(rank(time) <= 2)

duplicate_data
```

```{r pct carbon}
pct_carbon <- duplicate_data$carbon / 100
hist(pct_carbon, breaks = 30, xlab = "Proportion SOC")
```


### Maillard's study

A literature search study by Emilie Maillard gave the following measures of location and scale of SOC distributions in grasslands around the world [@maillard_increased_2017]:

- SOC stocks had a range of 4 to 1275 Mg C ha$^{-1}$ with a mean of 65 Mg C ha$^{-1}$
- Coefficient of variation ranged from 0.5 to 89\% with a mean of 22\%
- According to a regression analysis, variation is explainable (somewhat) by the scale of experiments and the depth of samples. Variation increases with depth and with the scale of the area under study. 



# References 