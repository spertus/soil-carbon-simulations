---
title: "Simulating SOC Measurement"
author: "Jake Spertus"
date: "October 31st, 2019"
output: html_notebook
header_includes:
  -\usepackage{amsmath}
  -\usepackage{amsfonts}
bibliography: soilcarbonstatistics.bib
---

```{r knitr setup, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r packages, message = FALSE, results = FALSE}
library(tidyverse)
source("sim_functions.R")
```

## Goals

This notebook serves to describe and implement a simulation to test the procedures currently used to measure soil organic carbon (SOC) against a known groundtruth. Questions to be answered include:

- With what accuracy can we recover true SOC on average (mean squared error)?
- Are estimates of SOC likely to center on the truth (bias)?
- Are estimates of uncertainty (standard errors) reflecting the true degree of uncertainty in the estimates (coverage of confidence intervals / type 1 error rate)? 


## Empirical Input: The Marin Data

Simulations will be based off of samples from Marin County collected by the Silver Lab at UC Berkeley.

```{r duplicate data, message = FALSE}
duplicate_data <- read_csv("mcp_duplicates.csv")
#drop samples that were rerun fewer or more than 2 times
no_dups <- names(which(table(duplicate_data$sample_name) < 2))
#also drop runs of atropine standards
duplicate_data <- duplicate_data %>%
  filter(!(sample_name %in% no_dups)) %>%
  filter(!(substr(sample_name, 1, 4) %in% c('atro','blan','Blan','dumm','Dumm'))) %>%
  group_by(sample_name) %>%
  filter(rank(time) <= 2)

duplicate_data
```

```{r pct carbon}
pct_carbon <- duplicate_data$carbon / 100
hist(pct_carbon, breaks = 30, xlab = "Proportion SOC")
```


## Simulating Percent SOC

As a first step, we simulate a "ground truth" (so to speak), which is a 3-dimensional grid on a rectangular prism that represents SOC concentration (in percent) over the plot of land to be studied. The dimensions are `latitude`, `longitude`, and `depth` (in that order). The scale is in meters. 

However, simulating a field of % SOC is not entirely straightforward. In particular we would like to generate numbers that:

- Are bounded between 0 and 1 (0% and 100% SOC).
- Allow for arbitrary correlation between points.
- Have an adjustable mean and variance.

Points 1 and 3 could be handled naturally using a Beta distribution, but point 2 is trickier. In order to generate a correlated field we will use Gaussian copulas: we take draws on $\mathbb{R}$ from a multivariate Gaussian distribution, and then map them back to $[0,1]$. The correlation structure is fixed by assuming stationarity and employing a particular variogram to define the correlations over distance. This is a standard technique in geostatistics, a model-based approach with applications in soil science (especially local estimation / the mapping of soils). 

Of course, variograms must themselves be estimated and variograms for SOC are kind of all over the place. [@paterson_variograms_2018] holds a compendium of variograms estimated from various studies of SOC, the vast majority of which were conducted on cropland. In this compilation, there are no SOC variograms on rangeland, and relatively few in the United States at all. I have decided to set the nugget variance equal to 0.005 and the sill variance equal to 0.02, with a range of 20 units (corresponding at a 10cm scale to 2m). This choice is poorly constrained to the best of my knowledge, but seems reasonable.

The idea here is to simulate the concentration of SOC in the topsoil (the first horizon, if you like) using a Gaussian copula and then extrapolate this to lower depths. In theory and in the field, we can sample anywhere on a plot so that %SOC comes from an infinite population. On a computer, we have a finite amount of memory to store a surface, so we must simulate it to some resolution. I have been using a $250 \times 600$ surface to represent a plot of land for sampling. If we take this as a standard 25m $\times$ 60m plot, we have a resolution of 10cm. We can consider a single point in the initial surface to represent the *average* %SOC in a $10 \times 10 \times 30$ centimeter cube. Note that 10 centimeters is a little bit larger than the diameter of a standard corer (7 cm in [@ryals_impacts_2014]).

```{r simulated surface}
source("sim_functions.R")


#Variogram input: Paterson et al 2018, "Variograms of soil properties..."
surface <- simulate_truth(size = c(250,600), nugget = .005, sill = .02, range = 20, intercept = .005, y_trend = TRUE, max_mean = .1)

#marginal histogram
sim_hist <- ggplot(data = surface, aes(z)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = mean(surface$z), size = 1.2, color = 'blue')
sim_hist

#surface
plot_surface(surface)
```


## Depth

The above simulations took place in 2 dimensions. The generated surface can be thought of as the average density of SOC in the topsoil (say the first 0-30cm). For some goals (like enhanced productivity, tilth, water retention, etc) understanding concentrations in the top soil may be enough. For sequestration in particular, deeper soil horizons matter. Thus we must consider additional horizons for realistic simulations. 

A typical field study might sample from 4 depths: for example 0-10, 10-30, 30-50, and 50-100cm [@ryals_impacts_2014]. Evidence suggests that SOC decreases exponentially with depth [@allen_review_2010]. 

Let's generically define some number of horizons, say 4, that represent discrete horizons (e.g. as above 0-10,10-30,...) and identify these with indices 0,1,2,3 where 0 is the surface and 3 is the deepest horizon. Let $z(x,y,0)$ be the %SOC at the surface then the %SOC at deeper horizons is generated as: 

$$z(x,y,d) = z(x,y,0) \exp(\gamma d)$$

The parameter $\gamma$ determines how fast %SOC declines with depth (typically $\gamma < 0$ and larger $|\gamma|$ means a faster decline).


```{r}
#as an alternative to exponential decrease one can explicitly specify the dropoff by specifying decrease = "manual" and the proportions argument
surface_depth <- add_depth_samples(surface = surface, increments = 4, decrease = "exponential", gamma = -1)
boxplot(z ~ depth, data = surface_depth)
```

## Bulk Density


<!---
The total Carbon in a plot of land, in Megagrams Carbon per hectare (Mg C / ha), is computed from both the percent carbon and the bulk density (a measure of the density of the soil). The bulk density is itself difficult to sample and compute. For the time being we will treat this as fixed and known. From the Silver lab report for California's Fourth Climate Change Assessment, average bulk density across a number of Californian sites was about 1.2 grams per cubic centimeter (g / cm$^3$), which is the number I will use.

Typical carbon stocks: 

- 53 Mg C / ha observed in the Sierra foothills, as observed in [Ryals et al 2014](https://www.sciencedirect.com/science/article/abs/pii/S003807171300312X). In the [CCA4 report](https://www.energy.ca.gov/sites/default/files/2019-07/Agriculture_CCCA4-CNRA-2018-002.pdf)
- The Silver lab reported stocks of 11 to 108 Mg C / ha across a range of sites in California, with a mean stock of 27 Mg C / ha.
!--->


# Sampling 

For our purposes, key features of Silver lab sampling are:

- Plots are 60 by 25 meters.
- Samples are taken along a transect with $n = 9$ samples per plot (sometimes $n=5$ samples per plot). It is not clear if these are randomized at all or merely start at one corner, however I believe the sampling falls under the category of systematic random sampling, i.e. the initial location is random (see figure from Allen et al below).
- Occasionally, plots are stratified and multiple transects will be taken (e.g. 3 transects with 3 samples each)
- Soils are collected using a 7 cm corer, at 4 depths: 0-10 cm, 10-30 cm, 30-50 cm, and 50-100 cm.


## Upper bounds for simple random sampling

We can use the *Hoeffding bound* to construct a finite sample confidence interval on the percent carbon estimated using simple random sampling. This bound is finite-sample valid and does not rely on any other assumptions other than the fact that percent carbon is in $[0,1]$. For samples $X_1, X_2, ... X_n$ drawn from a bounded population, the probability that the sample mean is $\epsilon$ far from the population mean is:

$$P(|\bar{X} - \mathbb{E}[X_1]| \geq \epsilon) \leq 2\exp\{-2 n \epsilon^2\} $$

If we would like a 95% confidence interval on the sample mean, then we can set the LHS equal to .05 and rearrange:

\begin{align}
.05 &\leq 2 \exp\{- 2 n \epsilon^2\}\\ 
\log(.025) &\leq -2 n \epsilon^2\\
\frac{\log(.025)}{-2\epsilon^2} &\leq n
\end{align}

Thus for a 95\% confidence interval with width $\epsilon$ we would need $\frac{\log(.025)}{-2\epsilon^2}$ samples. This is a lot. Below we plot $n$ as a function of $\epsilon$, the desired precision, on the $\log_{10}$ scale.

On the other hand, we could consider the *central limit theorem* and an upper bound on the population variance. Since $X_i \in [0,1]$, we have that $\sigma^2 \leq 1/4$. The central limit theorem gives an asymptotic distribution of the variation of the sample mean from the population mean:

$$\mathbb{P}(|\bar{X} - \mu| > \epsilon) = 2 \bigg (1 - \Phi \big (\frac{\epsilon}{\sigma / \sqrt{n}} \big ) \bigg ) \leq 2 \big (1-\Phi ( 4\epsilon \sqrt{n}) \big )$$
Therefore if we wish to bound deviations from the mean by $\epsilon$ to occur with probability less than $\alpha$ we can collect $n$ so that:

\begin{align}
2(1 - \Phi(4 \epsilon \sqrt{n})) &\leq \alpha\\
&\implies \\
\Phi(4 \epsilon \sqrt{n}) \geq 1 - \alpha/2\\
&\implies\\
n \geq \frac{\Phi^{-1}(1-\alpha/2)^2}{16 \epsilon^2}
\end{align}

```{r}
get_n_hoeffding <- function(epsilon, alpha = .05){
  #compute the hoeffding bound given a level of precision (epsilon) and a level of desired confidence (alpha)
  n <- log(alpha / 2) / (-2 * epsilon^2) 
  n
}
#population variance is 1/4 in the worst case for pct carbon
get_n_clt <- function(epsilon, alpha = .05, variance = 1/4){
  #compute the sample sizes needed using a wald interval
  n <- (qnorm(1 - alpha/2)^2) / (16 * epsilon^2)
  n
}

epsilon_grid <- seq(.01, .1, length.out = 100)
n_vs_epsilon <- data.frame(n_hoeffding = get_n_hoeffding(epsilon_grid), n_clt = get_n_clt(epsilon_grid), epsilon = epsilon_grid) %>%
  pivot_longer(cols = c("n_hoeffding","n_clt"))



ggplot(data = n_vs_epsilon, aes(x = epsilon, y = value, colour = name)) +
  geom_line() +
  theme_linedraw() +
  scale_y_log10() +
  labs(y = "n", x = "epsilon")
```

# Compositing

# Sample Preparation

# Measurement

Once samples have been taken, error in the assay itself remains a concern. The Silver lab takes duplicate measurements until two runs are within 10% of each other (this often happens after the first two runs of any given sample). These percent differences are computed as:

$$\delta = 100 * \bigg | \frac{x_1 - x_2}{\frac{1}{2}(x_1 + x_2)} \bigg |$$




# Analysis 

According to [Ryals et al 2014](https://www.sciencedirect.com/science/article/abs/pii/S003807171300312X), the carbon stock in a plot is computed by averaging the samples and then multiplying by the bulk density measurement. These averages are then treated as fixed and analyzed with ANOVA for final inference about treatment effects. 


# Simulations 

We first generate 10 plots (5 treatment, 5 control) using the simulation set up above. We first run simulations to test the type 1 error control when the sharp null hypothesis of no treatment effect for any units holds. In this setting all surfaces are generated in exactly the same way so that the true treatment effect is 0. We then check the power when the null hypothesis is wrong and there is a treatment effect. The specific alternative we consider is when there is an additive effect of 1% additional carbon across the entire plot.



# References 