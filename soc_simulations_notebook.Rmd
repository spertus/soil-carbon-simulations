---
title: "SOC Simulations"
author: "Jake Spertus"
date: "May 20th, 2020"
output: html_notebook
header_includes:
  -\usepackage{amsmath}
  -\usepackage{amsfonts}
  -\usepackage{color}
  -\newcommand{\indep}{\perp \!\!\! \perp}
bibliography: soilcarbonstatistics.bib
---

```{r knitr setup, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r packages, message = FALSE, results = FALSE}
library(tidyverse)
source("sim_functions.R")
```

## Goals

This notebook serves to describe and implement a simulation framework to test the procedures currently used to measure soil organic carbon (SOC) against a known groundtruth. 


# Notation

### The Surface

To begin we will consider %SOC as a two-dimensional surface defined over plots $\mathcal{P} \subset \mathbb{R}^2$, where $\mathbb{R}^2$ is a vector space with the usual Euclidean norm $\|u\| = \sqrt{u_1^2 + u_2^2}$ and inner product $\langle u,v \rangle = u_1v_1 + u_2v_2$. Typically, $\mathcal{P} \equiv [a,b]\times[c,d]$, a rectangular plot. For example in [@ryals_impacts_2014] we have a $25 \times 60$ meter plot so that $|a-b| = 25$ and $|c-d| = 60$. A typical element of $\mathcal{P}$ is an ordered pair $(x,y)$, which denotes the position of a point in the plot. Since we typically will consider a single plot, we can define (0,0) to be the lower left hand corner of the plot, so that points $(x,y)$ are relative to the position of the plot itself in $\mathbb{R}^2$. WLOG, we'll typically also take the orientation of the plot to be such that the $y$ direction indicates North/South (latitude) and the $x$ direction indicates East/West (longitude). For example, $(1,2)$ indicates a point 1 meter to the east and 2 meters to the north of the origin. 

Given any point $(x,y)$ there is an associated concentration of soil carbon (%SOC), which we will denote by $z(x,y)$. Note that %SOC is not truly defined at a precise point $(x,y)$ - there is considerable variance at ever smaller scales: a tiny piece of root may be 100\% carbon, while a patch of sand may be 0\% carbon. $z(x,y)$ is better conceptualized as an average over a small window centered at $(x,y)$. In a design-based framework (as we are considering here), $z(x,y)$ is a fixed, unknown number. Typically the parameter of interest is:

$$\mu \equiv \int_\mathcal{P} z(x,y) d\mathcal{P}  = \int_a^b \int_c^d z(x,y)~ dy ~dx$$

This is the "population average" %SOC over the plot $\mathcal{P}$. We wish to estimate $\mu$ using data. In field studies, data take the form of finite samples determined at specific values of $x$ and $y$: $\{z(x_1,y_1), z(x_2, y_2), ... z(x_n, y_n) \}$ often we'll denote the data $\{z_1, z_2, ... z_n\} \equiv \{z_i\}_{i=1}^n$ when the locations themselves are not super important. 

The properties of the data $\{z_i\}_{i=1}^n$ and estimators based on them depend on how the sampling locations $(x_1,y_1),...,(x_n,y_n)$ were chosen. For example, we can easily get an unbiased estimator by simple random sampling, choosing $x_i \sim U[a,b]$ and $y_i \sim U[c,d]$ independently where $U[a,b]$ denotes the uniform distribution on $[a,b]$. Chosen in this way we have an unbiased sample mean:

$$\mathbb{E}\bigg [\frac{1}{n} \sum_{i=1}^n z_i \bigg ] = \mu$$

Other sampling mechanisms (e.g. stratified sampling) can also satisfy unbiasedness and may have lower variance, which is very important since collecting samples is expensive. 


### Depth

In reality, %SOC is a defined in 3-dimensions because soil has depth. For sequestration research in particular depth is an important consideration. We will let $d$ denote depth and set $d=0$ to be the soil surface so that it is always defined relatively (as opposed to say, altitude). With depth taken into account, $\mathcal{P} \subset \mathbb{R}^3$ and typically $\mathcal{P} = [a,b] \times [c,d] \times [e,f]$ with $|e - f| = 1$ meter. $z(x,y,d)$ is the %SOC at position $(x,y)$ and depth $d$. The parameter of interest (with apologies for overloading $d$) is:

$$\mu = \int_\mathcal{P} z(x,y,d) d\mathcal{P} = \int_a^b \int_c^d \int_e^f z(x,y,d) ~dd ~dy ~dx$$

A typical sampling design will *not* randomly sample $d \sim U[e,f]$. Typically, $d$ is specified in advance to cover a discrete grid of depths $\mathcal{D} \equiv \{d_1, d_2,...d_m\}$. For example $m = 4$ and  $\mathcal{D} = \{.1, .3, .5, 1.0\}$ meters. Further, there is typically no random sampling in depth. At *every* surface point $(x_i, y_i)$ we gather samples $\{z(x_i, y_i, d_1), z(x_i, y_i, d_2), ..., z(x_i, y_i, d_m)\}$. For brevity we'll denote the sample at position $(x_i, y_i)$ and depth $d_j$ by $z_i(d_j)$. Furthermore, when dealing with the surface $z_i(d_1)$ we will often supress depth completely, simply writing $z_i(d_1) \equiv z_i$



# Simulation

### The Surface

We start by simulating a two-dimensional surface $\{z(x, y) | (x,y) \in \mathcal{P} \subset \mathbb{R}^2\}$ as random draws from a distrubition, and then add additional depths deterministically.

The function $z(x,y)$ representing %SOC over the plot $\mathcal{P}$ should satisfy a few properties in order to be realistic. At the same time, we want to be able to randomly generate an arbitrarily large number of surfaces. We seek a distribution $\mathcal{F}$ so that $z(x,y,d) \sim \mathcal{F}$ will satisfy these properties. Namely $z(x,y,d)$ should:

- Have a user-specifiable mean and variance.
- Be bounded between 0 and 1 (0% and 100% SOC).
- $z(x_i,y_i)$ should be close to $z(x_j, y_j)$ when $(x_i,y_i)$ is close to $(x_j,y_j)$.

Points 1 and 2 could be handled naturally using a Beta distribution, but point 3 is trickier. In order to generate a correlated field we will first generate draws from a multivariate Gaussian with a covariance matrix that makes draws close when points are close (point 3). We will then squash these draws back to the interval $[0,1]$ using the Gaussian CDF $\Phi(\cdot)$. The correlation structure in the Gaussian is fixed by assuming stationarity and employing a particular variogram to define the covariance over distance. This is a standard technique in geostatistics, a model-based approach with applications in soil science (especially local estimation / the mapping of soils). 


In symbols we define $\mathcal{P} \subset \mathbb{R}^2$ as a discrete $I \times J$ grid and generate a random length-$(I\cdot J)$ vector $\boldsymbol{\zeta} \sim \mathcal{N}_{I\cdot J}(\theta, \Sigma)$. Then $Z(x_i, y_j) \equiv \Phi(\zeta_{i\cdot j})$. The nature of realizations $z(x_i, y_j)$ depend on the parameters $\theta$ and $\Sigma$. 

- $\theta$ determines where realizations will be centered. For example we may have 3\% SOC on average in a rangeland plot, giving $\theta = \Phi^{-1}(.03) \approx -1.9$. $\theta$ can also exhibit a trend in $(x_i, y_j)$. In this case we will write $\theta(x_i,y_j)$. If there is a linear trend we might write $\theta(x_i,y_j) \equiv \beta_0 + \beta_x x_i + \beta_y y_j$. This might happen if the plot lies on a slope, for example, wherein ridges tend to have less carbon than valleys. 
- $\Sigma$ determines the variability of $z(x_i, y_j)$ and how similar close points are to each other. \textcolor{red}{Flesh this out more...}



Of course, variograms must themselves be estimated and variograms for SOC are kind of all over the place. [@paterson_variograms_2018] holds a compendium of variograms estimated from various studies of SOC, the vast majority of which were conducted on cropland. In this compilation, there are no SOC variograms on rangeland, and relatively few in the United States at all. I have decided to set the nugget variance equal to 0.005 and the sill variance equal to 0.02, with a range of 20 units (corresponding at a 10cm scale to 2m). This choice is poorly constrained to the best of my knowledge, but seems reasonable.

The idea here is to simulate the concentration of SOC in the topsoil (the first horizon, if you like) using a Gaussian copula and then extrapolate this to lower depths. In theory and in the field, we can sample anywhere on a plot so that %SOC comes from an infinite population. On a computer, we have a finite amount of memory to store a surface, so we must simulate it to some resolution. I have been using a $250 \times 600$ surface to represent a plot of land for sampling. If we take this as a standard 25m $\times$ 60m plot, we have a resolution of 10cm. We can consider a single point in the initial surface to represent the *average* %SOC in a $10 \times 10 \times 30$ centimeter cube. Note that 10 centimeters is a little bit larger than the diameter of a standard corer (7 cm in [@ryals_impacts_2014]).



```{r simulated surface}
source("sim_functions.R")


#Variogram input: Paterson et al 2018, "Variograms of soil properties..."
surface <- simulate_truth(size = c(250,600), nugget = .005, sill = .02, range = 20, intercept = .005, y_trend = TRUE, max_mean = .1)

#marginal histogram
sim_hist <- ggplot(data = surface, aes(z)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = mean(surface$z), size = 1.2, color = 'blue')
sim_hist

#surface
plot_surface(surface)
```


## Simulating depth

The above simulations took place in 2 dimensions. The generated surface can be thought of as the average density of SOC in the topsoil (say the first 0-30cm). For some goals (like enhanced productivity, tilth, water retention, etc) understanding concentrations in the top soil may be enough. For sequestration in particular, deeper soil horizons matter. Thus we must consider additional horizons for realistic simulations. 

A typical field study might sample from 4 depths: for example 0-10, 10-30, 30-50, and 50-100cm [@ryals_impacts_2014]. Evidence suggests that SOC decreases exponentially with depth [@allen_review_2010]. 

Let's generically define some number of horizons, say 4, that represent discrete horizons (e.g. as above 0-10,10-30,...) and identify these with indices 0,1,2,3 where 0 is the surface and 3 is the deepest horizon. Let $z(x,y,0)$ be the %SOC at the surface then the %SOC at deeper horizons is generated as: 

$$z(x,y,d) = z(x,y,0) \exp(\gamma d)$$

The parameter $\gamma$ determines how fast %SOC declines with depth (typically $\gamma < 0$ and larger $|\gamma|$ means a faster decline).


```{r}
#as an alternative to exponential decrease one can explicitly specify the dropoff by specifying decrease = "manual" and the proportions argument
surface_depth <- add_depth_samples(surface = surface, increments = 4, decrease = "exponential", gamma = -1)
boxplot(z ~ depth, data = surface_depth)
```




# Sampling 

We draw samples from a simulated surface using either simple random sampling or systematic random sampling. 

For simple random sampling pick $n$ sampling locations by drawing $X_i$ uniformly with replacement from $\{1,...,I\}$ and $Y_i$ uniformly with replacement from $\{1,...,J\}$. This picks out true samples $z(X_i, Y_i)$, where $z(X_i, Y_i)$ is random because $X_i$ and $Y_i$ are randomly chosen (not because $z(\cdot, \cdot)$ is a random fucntion). The data are realizations $z(x_i, y_i) \equiv z_i$.

For transect sampling, an initial we pick an initial $X_i$ from the left or right hand corner $X_i \sim \mbox{U}\{1,...a_{\mbox{limit}},b_{\mbox{limit}},...,I \}$ and an initial $Y_i$ from a band along the bottom of the plot $Y_i \sim \mbox{U}\{1,...c_{\mbox{limit}}\}$. The limits of the initial draw are chosen so that a regularly spaced transect can accomodate $n$ data points equally spaced across the diagonal of the plot. The rest of the points are then determined with no randomness by stepping along increments of the transect. Thus the sampling points and the samples SOC values $z(X_1,Y_1),...,z(X_n,Y_n)$ are random but not independent. 

For realizations $z(x_i,y_i)$ at the surface (topsoil), we have realizations at depth $z(x_i, y_i, d_j) \equiv z_i(d_j) = z(x_i,y_i) \cdot \exp(\gamma d_j)$, where again there is no randomness in $d_j$ (it is a deterministic grid).

```{r}
samples <- collect_sample(surface = surface_depth, design = "simple random sample", n_samp = 30)

topsoil_samples <- samples %>% filter(depth == 0) %>% pull(z)
```

# Compositing

Compositing is fairly straightforward in a simulation. Assuming perfect homogenization of composite samples and equal proportions compositing, a composite sample is just the mean of its constituent samples. Given samples $\{z_1,...z_n\}$ we bin them into $k$ groups of size $n/k$ (ideally $k$ should be a factor of $n$). The samples in each group are then physically mixed together to form composite samples $\{s_1, ... s_k\}$. Let $\{c_1,...,c_k\}$ be a set of sets, where each $c_i = \{c_{i1}, c_{i2},..., c_{i (n/k)}\}$ are the indices of the constituent samples of $s_i$. We assume that samples are mixed in equal proportions (*equal proportions compositing*), so that $s_i = \sum_{j \in c_i} \frac{k}{n} z_j$. One subtlty is if $n ~\%\%~ k \neq 0$, then we don't have $k$ composites of size $n/k$. As a simple solution we can just bundle the extra samples into an arbitrary composite, making that composite sample ``bigger" than the rest by $n ~\%\%~ k$.

```{r}
composited_samples <- composite_samples(topsoil_samples, k = 5)
```


# Measurement error

We are primarily interested in multiplicative measurement errors. If we have a composite samples $s_i = \sum_{j \in c_i} \frac{k}{n} z_j$ then take measured samples to be $s_i^* = s_i \delta_i$ where $\mathbb{E}[\delta_i]$ and $\delta_i$ and $s_i^*$ are independent.


We want to be able to simulate corrupting samples with measurement error. I would like to be able to simulate errors $\delta_i$ so that $\mathbb{E}[\delta_i] = 1$ or $\mathbb{E}[\delta_i] = 0$. I also want to fix the variance $\sigma_\delta^2$ and bounds $[a_\delta, b_\delta]$ of the distribution (note that it must be that $\sigma_\delta^2 \leq \frac{1}{4} (b_\delta - a_\delta)^2$). I will accomplish this by simulating $\delta_i$ from a centered and scaled beta distribution that is constrained to be symmetric. 

Here's how we will generate these samples given the parameters:

1) Draw $\delta^*_i \sim \mathcal{B}(\alpha,\alpha)$ where $\alpha = \left ( \frac{(a_\delta - b_\delta)^2}{8 \sigma_\delta^2} - \frac{1}{2} \right )$ 

2) Take 
$$\delta_i = \left (\delta_i^* - \frac{1}{2} \right ) |a_\delta - b_\delta| + \mathbb{E}[\delta_i]$$

For example if we want an error centered at 1 with bounds $[.8,1.2]$ and standard deviation $\sigma_\delta = .05$ then $\alpha = .4^2 / (8 * .05^2) - 1/2 = 7.5$ and $\delta_i = (\delta_i^* - 1/2)(.4) + 1$

```{r}
#measure once
measured_composite_samples <- perturb_measurements(true_samples = composited_samples, error_type = "multiplicative", error_bounds = c(.8, 1.2), error_sd = .05)

#measure_multiple_times (replicate the assay)
measured_composite_samples <- perturb_measurements(true_samples = composited_samples, error_type = "multiplicative", error_bounds = c(.8,1.2), error_sd = .05, replicates = 3) 
```

#### EcoCore and measurement error

[EcoCore analytic services](https://ecocore.nrel.colostate.edu/ecocore-elemental-analysis.html) reports a measurement error from DC-EA as a 2.5\% coefficient of variation assuming a mean %SOC of 2.5%. 


#### The Silver lab and measurement error

The Silver lab takes duplicate measurements until two runs are within 10% of each other. These percent differences are computed as:

$$\delta = 100 * \bigg | \frac{s_1 - s_2}{\frac{1}{2}(s_1 + s_2)} \bigg |$$

Therefore the measurement error of their samples is bounded by $\pm 10\%$. This is the interval I will use as a default in my simulations.


# Estimation and Inference

Recall that our overall goal is to estimate $\mu = \int_\mathcal{P} z(x,y,d) d\mathcal{P}$ using our measurements $\{s_i^*\}_{i=1}^k$. The sample mean of the measured composite samples is an unbiased estimator of $\mu$: $\mathbb{E}[\bar{s}^*] = \mu$. Furthermore it's variance is $\frac{\sigma^2 (1 + \sigma_\delta^2) }{n}  + \frac{\mu^2 \sigma_\delta^2}{k}$


Let's consider an example where we fix $\mu$, $\sigma^2$, and $\sigma_\delta^2$ based on the Marin carbon survey data from the Silver lab (see below). In one plot (sc4), we had $\hat{\mu} \approx .04$ as the average %SOC in topsoil (0-10cm), the estimated plot variance was $\hat{\sigma}^2 = 1.04 \times 10^{-4}$ and $\hat{\sigma}_\delta^2 = 1.02 \times 10^{-4}$ is the estimated measurement variance computed by taking a median of the variance of duplicate measurements. Given $n$ samples composited down to $k$, we have a variance of 

$$\mathbb{V}[\bar{s}^*] = \frac{(1.04 \times 10^{-4}) (1 + 1.02 \times 10^{-4})}{n} + \frac{.04^2 \times (1.02 \times 10^{-4})}{k} \approx \frac{1.0401\times10^{-4}}{n} + \frac{1.63\times10^{-7}}{k}$$

   

### Simulation experiment

We can run a Monte Carlo simulation experiment to see how the actual variance of our estimator stacks up against the theoretical variance above. Let's take $n=100$ and consider $k \in \{1,10,100\}$. The difference between $k = 1$ and $k = 100$ should theoretically be about:

$$\frac{(n-k)}{nk} \mu^2 \sigma_\delta^2 = \frac{99}{100} .04^2 (1\times 10^{-4}) = 1.6\times10^{-7}$$

Furthermore, the variance had we measured $n = 100$ samples is 

$$\frac{1.0401\times10^{-4} + 1.63\times10^{-7}}{100} = 1.042 \times 10^{-6}$$

```{r simulate measurement error, include = TRUE, echo = TRUE}
#put parameters on the same order as observed in Silver lab data 
#it's a stationary surface (no y_trend)
#the below parameters give a population variance of about sigma^2 = 3.8e-5 and mean mu = 0.04, as desired
sim_surface <- simulate_truth(size = c(250, 600), nugget= 4e-3, sill = 1e-2, range = 20, intercept = .04, y_trend = FALSE)
plot_surface(sim_surface)

run_simulations <- function(surface, k){
  samples <- collect_sample(surface = surface, design = "simple random sample", n_samp = 100) %>% pull(z)
  
  composites_list <- lapply(k, composite_samples, samples = samples)
  
  measured_samples_list <- lapply(composites_list, perturb_measurements, error_type = "multiplicative", error_bounds = c(.8, 1.2), error_sd = sqrt(1e-4)) %>%
    lapply(pull, "measurement")
  
  means_list <- lapply(measured_samples_list, mean)
  means <- unlist(means_list)
  names(means) <- k
  
  means
}

replicates <- replicate(n = 2000, run_simulations(surface = sim_surface, k = c(1,10,100))) %>%
  t() %>%
  as_tibble() 

replicates_long <- replicates %>% 
  pivot_longer(cols = everything(), names_to = "k", values_to = "estimate")

replicates_plot <- ggplot(data = replicates_long, aes(x = estimate)) + 
  geom_histogram() +
  facet_grid(~ k)

replicate_variance <- replicates_long %>%
  group_by(k) %>%
  summarize(variance = var(estimate)) %>%
  mutate(variance = signif(variance, 2))
replicate_variance
```

The simulations show a difference of $`r replicate_variance$variance[1] - replicate_variance$variance[3]`$. The percent variance reduction is  $`r (replicate_variance$variance[1] - replicate_variance$variance[3])`$.





# Empirical Inputs:


### The Marin Data

Simulations will be based off of samples from Marin County collected by the Silver Lab at UC Berkeley.

```{r duplicate data, message = FALSE}
duplicate_data <- read_csv("mcp_duplicates.csv")
#drop samples that were rerun fewer or more than 2 times
no_dups <- names(which(table(duplicate_data$sample_name) < 2))
#also drop runs of atropine standards
#only keep top soil samples
duplicate_data <- duplicate_data %>%
  filter(!(sample_name %in% no_dups)) %>%
  filter(!(substr(sample_name, 1, 4) %in% c('atro','blan','Blan','dumm','Dumm'))) %>%
  filter(str_ends(sample_name, "0-10")) %>% 
  mutate(carbon = carbon / 100, nitrogen = nitrogen / 100) %>%
  mutate(plot = str_split(sample_name, "-", simplify = T)[,1]) %>%
  mutate(plot = str_replace(plot, " ", "")) %>%
  group_by(sample_name) %>%
  filter(rank(time) <= 2) 

plot_mean <- duplicate_data %>% 
  summarize(mu = mean(carbon))

plot_variance <- duplicate_data %>%
  group_by(sample_name) %>%
  summarize(plot = first(plot),  carbon = mean(carbon)) %>%
  ungroup() %>%
  group_by(plot) %>%
  summarize(plot_variance = var(carbon)) %>%
  ungroup() 

measurement_variance <- duplicate_data %>%
  group_by(sample_name) %>%
  summarize(plot = first(plot), variance = var(carbon)) %>%
  ungroup() %>%
  group_by(plot) %>%
  summarize(median_measurement_error = median(variance))

combined_variance <- plot_variance %>%
  inner_join(measurement_variance, by = "plot")

plot_vs_measurement_variance_plot <- ggplot(combined_variance, aes(x = plot_variance, y = median_measurement_error)) +
  geom_point() 
```



### Maillard's study

A literature search study by Emilie Maillard gave the following measures of location and scale of SOC distributions in grasslands around the world [@maillard_increased_2017]:

- SOC stocks had a range of 4 to 1275 Mg C ha$^{-1}$ with a mean of 65 Mg C ha$^{-1}$
- Coefficient of variation ranged from 0.5 to 89\% with a mean of 22\%
- According to a regression analysis, variation is explainable (somewhat) by the scale of experiments and the depth of samples. Variation increases with depth and with the scale of the area under study. 



# References 